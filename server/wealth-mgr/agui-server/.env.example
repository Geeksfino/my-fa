# Server Configuration
PORT=3000
HOST=0.0.0.0
NODE_ENV=development

# LLM Provider Configuration
LLM_PROVIDER=litellm
LITELLM_ENDPOINT=http://localhost:4000/v1
LITELLM_MODEL=deepseek-chat
LITELLM_API_KEY=your-api-key-here

# Alternative: Direct DeepSeek
# LLM_PROVIDER=deepseek
# DEEPSEEK_API_KEY=your-deepseek-key
# DEEPSEEK_MODEL=deepseek-chat

# MCP Server Configuration (optional)
# Enable to allow LLM to call MCP-UI tools

# Option 1: HTTP Transport (Recommended for cloud/production deployment)
# Use this when both servers run as independent long-running processes
MCP_SERVER_URL=http://localhost:3100/mcp

# Option 2: Stdio Transport (for local/development)
# Use this for local development where agui-test-server spawns mcpui-test-server as a subprocess
# Note: Uncomment the following lines and comment out MCP_SERVER_URL to use stdio
# MCP_SERVER_COMMAND=node
# MCP_SERVER_ARGS=../mcpui-test-server/dist/server.js

# Agent Configuration
DEFAULT_AGENT=scenario
SCENARIO_DIR=./src/scenarios
SCENARIO_DELAY_MS=200

# SSE Configuration
SSE_RETRY_MS=3000
SSE_HEARTBEAT_MS=30000

# Logging
LOG_LEVEL=info
LOG_PRETTY=true

# CORS
CORS_ORIGIN=*
